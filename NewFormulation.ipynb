{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Constants =====\n",
    "# LOSS_RATE = 0  # Default\n",
    "LOSS_RATE = -1\n",
    "# LOSS_RATE = -1e-3\n",
    "\n",
    "GAMMA = 0.9999  # Default\n",
    "# GAMMA = 0.99\n",
    "# GAMMA = 1 - 1e-3\n",
    "\n",
    "N_BINS = 1\n",
    "N_GOALS = 10\n",
    "N_TASKS = 50\n",
    "\n",
    "PLANNING_FALLACY_CONST = 1  # Default\n",
    "# PLANNING_FALLACY_CONST = 1.39\n",
    "\n",
    "# SLACK_REWARD = np.NINF\n",
    "# SLACK_REWARD = 1\n",
    "# SLACK_REWARD = 1e-1\n",
    "# SLACK_REWARD = 1e-2\n",
    "SLACK_REWARD = 1e-3\n",
    "\n",
    "TIME_SCALE = 1  # Default\n",
    "# TIME_SCALE = 60\n",
    "\n",
    "VALUE_SCALE = 1  # Default\n",
    "# VALUE_SCALE = 10\n",
    "\n",
    "# UNIT_PENALTY = 10\n",
    "UNIT_PENALTY = 1\n",
    "# UNIT_PENALTY = .1\n",
    "# UNIT_PENALTY = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from todolistMDP.new_to_do_list import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: Task A11 Time Estimate: 3\n",
      "Item: Task A12 Time Estimate: 2\n",
      "Item: Task A13 Time Estimate: 2\n",
      "Item: SG A1 Time Estimate: 7\n",
      "Item: Task A21 Time Estimate: 3\n",
      "Item: Task A22 Time Estimate: 1\n",
      "Item: Task A23 Time Estimate: 1\n",
      "Item: SG A2 Time Estimate: 5\n",
      "Item: Goal A Time Estimate: 12\n",
      "Item: Task B11 Time Estimate: 4\n",
      "Item: Task B12 Time Estimate: 2\n",
      "Item: SG B1 Time Estimate: 6\n",
      "Item: Task B21 Time Estimate: 2\n",
      "Item: Task B22 Time Estimate: 2\n",
      "Item: Task B23 Time Estimate: 1\n",
      "Item: Task B24 Time Estimate: 10\n",
      "Item: Task B25 Time Estimate: 2\n",
      "Item: SG B2 Time Estimate: 17\n",
      "Item: Goal B Time Estimate: 23\n",
      "Item: Task C11 Time Estimate: 1\n",
      "Item: Task C12 Time Estimate: 2\n",
      "Item: SG C1 Time Estimate: 3\n",
      "Item: Task C21 Time Estimate: 50\n",
      "Item: Task C22 Time Estimate: 400\n",
      "Item: Task C23 Time Estimate: 50\n",
      "Item: Task C24 Time Estimate: 2\n",
      "Item: SG C2 Time Estimate: 502\n",
      "Item: Goal C Time Estimate: 505\n"
     ]
    }
   ],
   "source": [
    "d_bm_realistic = [\n",
    "    Item(\n",
    "        description=\"Goal A\",\n",
    "        completed=False,\n",
    "        intrinsic_reward=70,\n",
    "        essential=True,\n",
    "        deadline=12,\n",
    "        time_est=TIME_SCALE * 12,\n",
    "        value=1000,\n",
    "        today=True,\n",
    "        children=[\n",
    "            Item(\"SG A1\", time_est=TIME_SCALE * 7, essential=True, importance=30, value=1000, intrinsic_reward=40, today=True,\n",
    "                 completed=False, deadline=12, children=[\n",
    "                Item(\"Task A11\", time_est=TIME_SCALE * 3, essential=True, importance=60, value=1000, intrinsic_reward=10, today=True,\n",
    "                     completed=False, deadline=12),\n",
    "                Item(\"Task A12\", time_est=TIME_SCALE * 2, essential=True, importance=20, value=1000, intrinsic_reward=15, today=True,\n",
    "                     completed=False, deadline=12),\n",
    "                Item(\"Task A13\", time_est=TIME_SCALE * 2, essential=True, importance=20, value=1000, intrinsic_reward=15, today=True,\n",
    "                     completed=False, deadline=12)\n",
    "            ]),\n",
    "            Item(\"SG A2\", time_est=TIME_SCALE * 5, essential=True, importance=30, value=1000, intrinsic_reward=30, today=True,\n",
    "                 completed=False, deadline=12, children=[\n",
    "                Item(\"Task A21\", time_est=TIME_SCALE * 3, essential=True, importance=60, value=1000, intrinsic_reward=20, today=True,\n",
    "                     completed=False, deadline=12),\n",
    "                Item(\"Task A22\", time_est=TIME_SCALE * 1, essential=True, importance=30, value=1000, intrinsic_reward=2, today=True,\n",
    "                     completed=False, deadline=12),\n",
    "                Item(\"Task A23\", time_est=TIME_SCALE * 1, essential=True, importance=10, value=1000, intrinsic_reward=8, today=True,\n",
    "                     completed=False, deadline=12)\n",
    "                 ])\n",
    "        ]),\n",
    "        \n",
    "    Item(\n",
    "        description=\"Goal B\",\n",
    "        completed=False,\n",
    "        intrinsic_reward=200,\n",
    "        essential=True,\n",
    "        deadline=50,\n",
    "        time_est=TIME_SCALE * 23,\n",
    "        value=500,\n",
    "        today=True,\n",
    "        children=[\n",
    "            Item(\"SG B1\", time_est=TIME_SCALE * 6, essential=True, importance=50, value=500, intrinsic_reward=100,\n",
    "                 today=True,\n",
    "                 completed=False, deadline=50, children=[\n",
    "                    Item(\"Task B11\", time_est=TIME_SCALE * 4, essential=True, importance=90, value=500,\n",
    "                         intrinsic_reward=80, today=True,\n",
    "                         completed=False,deadline=50),\n",
    "                    Item(\"Task B12\", time_est=TIME_SCALE * 2, essential=True, importance=10, value=500,\n",
    "                         intrinsic_reward=20, today=True,\n",
    "                         completed=False,deadline=50)\n",
    "                ]),\n",
    "            Item(\"SG B2\", time_est=TIME_SCALE * 17, essential=True, importance=50, value=500, intrinsic_reward=100,\n",
    "                 today=True,\n",
    "                 completed=False, deadline=50, children=[\n",
    "                    Item(\"Task B21\", time_est=TIME_SCALE * 2, essential=True, importance=20, value=500,\n",
    "                         intrinsic_reward=20, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                    Item(\"Task B22\", time_est=TIME_SCALE * 2, essential=True, importance=60, value=500,\n",
    "                         intrinsic_reward=10, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                     Item(\"Task B23\", time_est=TIME_SCALE * 1, essential=True, importance=2, value=500,\n",
    "                         intrinsic_reward=10, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                     Item(\"Task B24\", time_est=TIME_SCALE * 10, essential=True, importance=15, value=500,\n",
    "                         intrinsic_reward=40, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                     Item(\"Task B25\", time_est=TIME_SCALE * 2, essential=True, importance=3, value=500,\n",
    "                         intrinsic_reward=20, today=True,\n",
    "                         completed=False, deadline=50)\n",
    "                 ])\n",
    "        ]),\n",
    "    Item(\n",
    "        description=\"Goal C\",\n",
    "        completed=False,\n",
    "        intrinsic_reward=100,\n",
    "        essential=True,\n",
    "        deadline=50,\n",
    "        time_est=TIME_SCALE * 505,\n",
    "        value=5000,\n",
    "        today=True,\n",
    "        children=[\n",
    "            Item(\"SG C1\", time_est=TIME_SCALE * 3, essential=True, importance=20, value=5000, intrinsic_reward=10,\n",
    "                 today=True,\n",
    "                 completed=False, deadline=50, children=[\n",
    "                    Item(\"Task C11\", time_est=TIME_SCALE * 1, essential=True, importance=60, value=5000,\n",
    "                         intrinsic_reward=5, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                    Item(\"Task C12\", time_est=TIME_SCALE * 2, essential=True, importance=40, value=5000,\n",
    "                         intrinsic_reward=5, today=True,\n",
    "                         completed=False, deadline=50)\n",
    "                ]),\n",
    "            Item(\"SG C2\", time_est=TIME_SCALE * 502, essential=True, importance=80, value=5000, intrinsic_reward=90,\n",
    "                 today=True,\n",
    "                 completed=False, deadline=50, children=[\n",
    "                    Item(\"Task C21\", time_est=TIME_SCALE * 50, essential=True, importance=20, value=5000,\n",
    "                         intrinsic_reward=50, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                    Item(\"Task C22\", time_est=TIME_SCALE * 400, essential=True, importance=60, value=5000,\n",
    "                         intrinsic_reward=10, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                    Item(\"Task C23\", time_est=TIME_SCALE * 50, essential=True, importance=10, value=5000,\n",
    "                         intrinsic_reward=20, today=True,\n",
    "                         completed=False, deadline=50),\n",
    "                    Item(\"Task C24\", time_est=TIME_SCALE * 2, essential=True, importance=10, value=5000,\n",
    "                         intrinsic_reward=10, today=True,\n",
    "                         completed=False, deadline=50)\n",
    "                 ])\n",
    "        ])\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: Root Time Estimate: 540\n"
     ]
    }
   ],
   "source": [
    "main = MainToDoList(d_bm_realistic, gamma=GAMMA, loss_rate= LOSS_RATE, num_bins=N_BINS, penalty_rate=PLANNING_FALLACY_CONST,slack_reward_rate=SLACK_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(main.nodes['Goal A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root': 0,\n",
       " 'Goal A': 0,\n",
       " 'SG A1': 0,\n",
       " 'Task A11': 0,\n",
       " 'Task A12': 0,\n",
       " 'Task A13': 0,\n",
       " 'Task A21': 0,\n",
       " 'Task A22': 0,\n",
       " 'Task A23': 0,\n",
       " 'SG A2': 0,\n",
       " 'SG B1': 0,\n",
       " 'Task B11': 0,\n",
       " 'Task B12': 0,\n",
       " 'Task B21': 0,\n",
       " 'Task B22': 0,\n",
       " 'Task B23': 0,\n",
       " 'Task B24': 0,\n",
       " 'Task B25': 0,\n",
       " 'SG B2': 0,\n",
       " 'SG C1': 0,\n",
       " 'Task C11': 0,\n",
       " 'Task C12': 0,\n",
       " 'Task C21': 0,\n",
       " 'Task C22': 0,\n",
       " 'Task C23': 0,\n",
       " 'Task C24': 0,\n",
       " 'SG C2': 0,\n",
       " 'Goal B': 0,\n",
       " 'Goal C': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** In MainToDoList solve ***********\n",
      "*********** MDP ***********\n",
      "Solving goal as: Root\n",
      "Root Val: 6500\n",
      "Solving for following items\n",
      "Goal A, Imp: 0.15384615384615385, Int: 70, Ess: False\n",
      "Goal B, Imp: 0.07692307692307693, Int: 200, Ess: False\n",
      "Goal C, Imp: 0.7692307692307693, Int: 100, Ess: False\n",
      "Value: 6500\n",
      "Policy:\n",
      "Action: 0\n",
      "s, t: ((0, 0, 0), 0), a: 0, s_, t_: ((1, 0, 0), 0)\n",
      "Action: 1\n",
      "s, t: ((1, 0, 0), 0), a: 1, s_, t_: ((1, 1, 0), 12)\n",
      "Slack Action Chosen. Action: 2, q: -379.2464321779721 <= 10.000000000001101\n",
      "Q value\n",
      "Goal A, reward: 1058.006597800495 Q: 2355.099232526975\n",
      "Goal B, reward: 677.0252822988516 Q: 419.3336958913499\n",
      "Goal C, reward: -381.519809102973 Q: -134.41978548949487\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -5778.796625648469\n",
      "Sum Of all Values: 6500\n",
      "Goal A: r: 1058.006597800495, pr: 2670.5808148251026, new_pr: 3728.5874126255976\n",
      "Scale: 1.1 Optimal Reward: -1058.006597800495, Bias: 3834.388072405647\n",
      "Goal B: r: 677.0252822988516, pr: 960.3181715777228, new_pr: 1637.3434538765744\n",
      "Scale: 1.1 Optimal Reward: -2612.7908189344766, Bias: 3834.388072405647\n",
      "Goal C: r: -381.519809102973, pr: 1515.5889426008007, new_pr: 1134.0691334978278\n",
      "Scale: 1.1 Optimal Reward: -2107.9992089134967, Bias: 3834.388072405647\n",
      "\n",
      "Total sum of pseudo-rewards: 6500.00\n",
      "\n",
      "Optimal Rewards\n",
      "Goal A 3728.5874126255976\n",
      "Goal B 1637.3434538765744\n",
      "Goal C 1134.0691334978278\n",
      "*********** MDP ***********\n",
      "Solving goal as: Goal A\n",
      "Goal A Val: 3728.5874126255976\n",
      "Solving for following items\n",
      "SG A1, Imp: 100, Int: 40, Ess: True\n",
      "SG A2, Imp: 100, Int: 30, Ess: True\n",
      "Value: 3728.5874126255976\n",
      "Policy:\n",
      "Action: 1\n",
      "s, t: ((0, 0), 0), a: 1, s_, t_: ((0, 1), 0)\n",
      "Action: 0\n",
      "s, t: ((0, 1), 0), a: 0, s_, t_: ((1, 1), 5)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "SG A1, reward: 33.002099650035 Q: 3783.9637884090744\n",
      "SG A2, reward: 25.000999900005 Q: 3784.7100935408375\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -58.749404681802844\n",
      "Sum Of all Values: 3728.5874126255976\n",
      "SG A1: r: 33.002099650035, pr: 1830.4810838527926, new_pr: 1863.4831835028276\n",
      "Scale: 1.1 Optimal Reward: -33.748404781797944, Bias: 1867.6043291127703\n",
      "SG A2: r: 25.000999900005, pr: 1840.1032292227649, new_pr: 1865.1042291227698\n",
      "Scale: 1.1 Optimal Reward: -25.0009999000049, Bias: 1867.6043291127703\n",
      "\n",
      "Total sum of pseudo-rewards: 3728.59\n",
      "\n",
      "Optimal Rewards\n",
      "SG A2 1865.1042291227698\n",
      "SG A1 1863.4831835028276\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG A1\n",
      "SG A1 Val: 1863.4831835028276\n",
      "Solving for following items\n",
      "Task A11, Imp: 60, Int: 10, Ess: True\n",
      "Task A12, Imp: 20, Int: 15, Ess: True\n",
      "Task A13, Imp: 20, Int: 15, Ess: True\n",
      "Value: 1863.4831835028276\n",
      "Policy:\n",
      "Action: 1\n",
      "s, t: ((0, 0, 0), 0), a: 1, s_, t_: ((0, 1, 0), 0)\n",
      "Action: 2\n",
      "s, t: ((0, 1, 0), 0), a: 2, s_, t_: ((0, 1, 1), 2)\n",
      "Action: 0\n",
      "s, t: ((0, 1, 1), 2), a: 0, s_, t_: ((1, 1, 1), 4)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "Task A11, reward: 7.00029999 Q: 1895.541729840631\n",
      "Task A12, reward: 13.0001 Q: 1895.733002430959\n",
      "Task A13, reward: 13.0001 Q: 1895.733002430959\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -33.191772580328006\n",
      "Sum Of all Values: 1863.4831835028276\n",
      "Task A11: r: 7.00029999, pr: 614.4204812787019, new_pr: 621.420781268702\n",
      "Scale: 1.1 Optimal Reward: -7.191572580328057, Bias: 622.3312111170628\n",
      "Task A12: r: 13.0001, pr: 608.0311011170628, new_pr: 621.0312011170628\n",
      "Scale: 1.1 Optimal Reward: -13.000099999999975, Bias: 622.3312111170628\n",
      "Task A13: r: 13.0001, pr: 608.0311011170628, new_pr: 621.0312011170628\n",
      "Scale: 1.1 Optimal Reward: -13.000099999999975, Bias: 622.3312111170628\n",
      "\n",
      "Total sum of pseudo-rewards: 1863.48\n",
      "\n",
      "Optimal Rewards\n",
      "Task A11 621.420781268702\n",
      "Task A12 621.0312011170628\n",
      "Task A13 621.0312011170628\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A11\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A12\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A13\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG A2\n",
      "SG A2 Val: 1865.1042291227698\n",
      "Solving for following items\n",
      "Task A21, Imp: 60, Int: 20, Ess: True\n",
      "Task A22, Imp: 30, Int: 2, Ess: True\n",
      "Task A23, Imp: 10, Int: 8, Ess: True\n",
      "Value: 1865.1042291227698\n",
      "Policy:\n",
      "Action: 2\n",
      "s, t: ((0, 0, 0), 0), a: 2, s_, t_: ((0, 0, 1), 0)\n",
      "Action: 1\n",
      "s, t: ((0, 0, 1), 0), a: 1, s_, t_: ((0, 1, 1), 1)\n",
      "Action: 0\n",
      "s, t: ((0, 1, 1), 1), a: 0, s_, t_: ((1, 1, 1), 2)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "Task A21, reward: 17.00029999 Q: 1889.3560995899034\n",
      "Task A22, reward: 1.0 Q: 1889.7274270279925\n",
      "Task A23, reward: 7.0 Q: 1889.7280270279925\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -25.372827428089295\n",
      "Sum Of all Values: 1865.1042291227698\n",
      "Task A21: r: 17.00029999, pr: 603.5618962636578, new_pr: 620.5621962536578\n",
      "Scale: 1.1 Optimal Reward: -17.372227428089218, Bias: 622.671346434556\n",
      "Task A22: r: 1.0, pr: 621.5706864345559, new_pr: 622.5706864345559\n",
      "Scale: 1.1 Optimal Reward: -1.0006000000000768, Bias: 622.671346434556\n",
      "Task A23: r: 7.0, pr: 614.9713464345559, new_pr: 621.9713464345559\n",
      "Scale: 1.1 Optimal Reward: -7.0, Bias: 622.671346434556\n",
      "\n",
      "Total sum of pseudo-rewards: 1865.10\n",
      "\n",
      "Optimal Rewards\n",
      "Task A22 622.5706864345559\n",
      "Task A23 621.9713464345559\n",
      "Task A21 620.5621962536578\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A21\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A22\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task A23\n",
      "*********** MDP ***********\n",
      "Solving goal as: Goal B\n",
      "Goal B Val: 1637.3434538765744\n",
      "Solving for following items\n",
      "SG B1, Imp: 100, Int: 100, Ess: True\n",
      "SG B2, Imp: 100, Int: 100, Ess: True\n",
      "Value: 1637.3434538765744\n",
      "Policy:\n",
      "Action: 0\n",
      "s, t: ((0, 0), 0), a: 0, s_, t_: ((1, 0), 0)\n",
      "Action: 1\n",
      "s, t: ((1, 0), 0), a: 1, s_, t_: ((1, 1), 6)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "SG B1, reward: 94.001499800015 Q: 1813.3265906698741\n",
      "SG B2, reward: 83.01359320237938 Q: 1811.4176139099532\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -178.92406976231518\n",
      "Sum Of all Values: 1637.3434538765744\n",
      "SG B1: r: 94.001499800015, pr: 725.1707690263469, new_pr: 819.1722688263619\n",
      "Scale: 1.1 Optimal Reward: -94.00149980001493, Bias: 828.5724188063633\n",
      "SG B2: r: 83.01359320237938, pr: 735.157591847833, new_pr: 818.1711850502124\n",
      "Scale: 1.1 Optimal Reward: -84.92256996230026, Bias: 828.5724188063633\n",
      "\n",
      "Total sum of pseudo-rewards: 1637.34\n",
      "\n",
      "Optimal Rewards\n",
      "SG B1 819.1722688263619\n",
      "SG B2 818.1711850502124\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG B1\n",
      "SG B1 Val: 819.1722688263619\n",
      "Solving for following items\n",
      "Task B11, Imp: 90, Int: 80, Ess: True\n",
      "Task B12, Imp: 10, Int: 20, Ess: True\n",
      "Value: 819.1722688263619\n",
      "Policy:\n",
      "Action: 1\n",
      "s, t: ((0, 0), 0), a: 1, s_, t_: ((0, 1), 0)\n",
      "Action: 0\n",
      "s, t: ((0, 1), 0), a: 0, s_, t_: ((1, 1), 2)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "Task B11, reward: 76.000599960001 Q: 912.838150065826\n",
      "Task B12, reward: 18.0001 Q: 912.9939431643343\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -94.15649305850923\n",
      "Sum Of all Values: 819.1722688263619\n",
      "Task B11: r: 76.000599960001, pr: 330.5998232510003, new_pr: 406.6004232110013\n",
      "Scale: 1.1 Optimal Reward: -76.15639305850925, Bias: 414.3718556153605\n",
      "Task B12: r: 18.0001, pr: 394.57174561536056, new_pr: 412.57184561536053\n",
      "Scale: 1.1 Optimal Reward: -18.000099999999975, Bias: 414.3718556153605\n",
      "\n",
      "Total sum of pseudo-rewards: 819.17\n",
      "\n",
      "Optimal Rewards\n",
      "Task B12 412.57184561536053\n",
      "Task B11 406.6004232110013\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B11\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B12\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG B2\n",
      "SG B2 Val: 818.1711850502124\n",
      "Solving for following items\n",
      "Task B21, Imp: 20, Int: 20, Ess: True\n",
      "Task B22, Imp: 60, Int: 10, Ess: True\n",
      "Task B23, Imp: 2, Int: 10, Ess: True\n",
      "Task B24, Imp: 15, Int: 40, Ess: True\n",
      "Task B25, Imp: 3, Int: 20, Ess: True\n",
      "Value: 818.1711850502124\n",
      "Policy:\n",
      "Action: 0\n",
      "s, t: ((0, 0, 0, 0, 0), 0), a: 0, s_, t_: ((1, 0, 0, 0, 0), 0)\n",
      "Action: 4\n",
      "s, t: ((1, 0, 0, 0, 0), 0), a: 4, s_, t_: ((1, 0, 0, 0, 1), 2)\n",
      "Action: 2\n",
      "s, t: ((1, 0, 0, 0, 1), 2), a: 2, s_, t_: ((1, 0, 1, 0, 1), 4)\n",
      "Action: 1\n",
      "s, t: ((1, 0, 1, 0, 1), 4), a: 1, s_, t_: ((1, 1, 1, 0, 1), 5)\n",
      "Action: 3\n",
      "s, t: ((1, 1, 1, 0, 1), 5), a: 3, s_, t_: ((1, 1, 1, 1, 1), 7)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "Task B21, reward: 18.0001 Q: 900.5712404088326\n",
      "Task B22, reward: 8.0001 Q: 900.5662415086927\n",
      "Task B23, reward: 9.0 Q: 900.5712402088527\n",
      "Task B24, reward: 30.004498800209973 Q: 899.8854219676184\n",
      "Task B25, reward: 18.0001 Q: 900.5712404088326\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -83.695616341544\n",
      "Sum Of all Values: 818.1711850502124\n",
      "Task B21: r: 18.0001, pr: 145.64620284514018, new_pr: 163.64630284514018\n",
      "Scale: 1.1 Optimal Reward: -18.000099999999975, Bias: 165.44631284514017\n",
      "Task B22: r: 8.0001, pr: 156.64070405498623, new_pr: 164.64080405498623\n",
      "Scale: 1.1 Optimal Reward: -8.005098900139956, Bias: 165.44631284514017\n",
      "Task B23: r: 9.0, pr: 155.54631262516224, new_pr: 164.54631262516224\n",
      "Scale: 1.1 Optimal Reward: -9.000000199979922, Bias: 165.44631284514017\n",
      "Task B24: r: 30.004498800209973, pr: 131.68696387957357, new_pr: 161.69146267978354\n",
      "Scale: 1.1 Optimal Reward: -30.690317241424168, Bias: 165.44631284514017\n",
      "Task B25: r: 18.0001, pr: 145.64620284514018, new_pr: 163.64630284514018\n",
      "Scale: 1.1 Optimal Reward: -18.000099999999975, Bias: 165.44631284514017\n",
      "\n",
      "Total sum of pseudo-rewards: 818.17\n",
      "\n",
      "Optimal Rewards\n",
      "Task B22 164.64080405498623\n",
      "Task B23 164.54631262516224\n",
      "Task B21 163.64630284514018\n",
      "Task B25 163.64630284514018\n",
      "Task B24 161.69146267978354\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B21\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B22\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B23\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B24\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task B25\n",
      "*********** MDP ***********\n",
      "Solving goal as: Goal C\n",
      "Goal C Val: 1134.0691334978278\n",
      "Solving for following items\n",
      "SG C1, Imp: 100, Int: 10, Ess: True\n",
      "SG C2, Imp: 100, Int: 90, Ess: True\n",
      "Value: 1134.0691334978278\n",
      "Policy:\n",
      "Slack Action Chosen. Action: 0, q: -390.02547099346054 <= 10.000000000001101\n",
      "Q value\n",
      "SG C1, reward: 7.00029999 Q: -390.02547099346054\n",
      "SG C2, reward: -399.6318962540837 Q: -390.60913071092426\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -408.00300544030324\n",
      "Sum Of all Values: 1134.0691334978278\n",
      "SG C1: r: 7.00029999, pr: 540.0236697913148, new_pr: 547.0239697813148\n",
      "Scale: 1.1 Optimal Reward: -407.0257709834616, Bias: 987.7520178731226\n",
      "SG C2: r: -399.6318962540837, pr: 986.6770599705967, new_pr: 587.045163716513\n",
      "Scale: 1.1 Optimal Reward: -0.9772344568416642, Bias: 987.7520178731226\n",
      "\n",
      "Total sum of pseudo-rewards: 1134.07\n",
      "\n",
      "Optimal Rewards\n",
      "SG C2 587.045163716513\n",
      "SG C1 547.0239697813148\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG C1\n",
      "SG C1 Val: 547.0239697813148\n",
      "Solving for following items\n",
      "Task C11, Imp: 60, Int: 5, Ess: True\n",
      "Task C12, Imp: 40, Int: 5, Ess: True\n",
      "Value: 547.0239697813148\n",
      "Policy:\n",
      "Action: 0\n",
      "s, t: ((0, 0), 0), a: 0, s_, t_: ((1, 0), 0)\n",
      "Action: 1\n",
      "s, t: ((1, 0), 0), a: 1, s_, t_: ((1, 1), 1)\n",
      "Slack Action Chosen. Action: None, q: 0 <= 10.000000000001101\n",
      "Q value\n",
      "Task C11, reward: 4.0 Q: 553.9690673743366\n",
      "Task C12, reward: 3.0000999999999998 Q: 553.9138704975983\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -7.055296876738339\n",
      "Sum Of all Values: 547.0239697813148\n",
      "Task C11: r: 4.0, pr: 269.4923481728635, new_pr: 273.4923481728635\n",
      "Scale: 1.1 Optimal Reward: -4.0, Bias: 273.8923481728635\n",
      "Task C12: r: 3.0000999999999998, pr: 270.5315216084513, new_pr: 273.5316216084513\n",
      "Scale: 1.1 Optimal Reward: -3.055296876738339, Bias: 273.8923481728635\n",
      "\n",
      "Total sum of pseudo-rewards: 547.02\n",
      "\n",
      "Optimal Rewards\n",
      "Task C12 273.5316216084513\n",
      "Task C11 273.4923481728635\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C11\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C12\n",
      "*********** MDP ***********\n",
      "Solving goal as: SG C2\n",
      "SG C2 Val: 587.045163716513\n",
      "Solving for following items\n",
      "Task C21, Imp: 20, Int: 50, Ess: True\n",
      "Task C22, Imp: 60, Int: 10, Ess: True\n",
      "Task C23, Imp: 10, Int: 20, Ess: True\n",
      "Task C24, Imp: 10, Int: 10, Ess: True\n",
      "Value: 587.045163716513\n",
      "Policy:\n",
      "Slack Action Chosen. Action: 3, q: -398.56436387497615 <= 10.000000000001101\n",
      "Q value\n",
      "Task C21, reward: 0.12230423008826818 Q: -398.60424207074607\n",
      "Task C22, reward: -382.12482552748196 Q: -401.63953799332563\n",
      "Task C23, reward: -29.877695769911732 Q: -398.75984493293464\n",
      "Task C24, reward: 8.0001 Q: -398.56436387497615\n",
      "****** In compute_start_state_pseudo_rewards ******\n",
      "sum_pr: -1233.6878718046814\n",
      "Sum Of all Values: 587.045163716513\n",
      "Task C21: r: 0.12230423008826818, pr: 137.39628401132302, new_pr: 137.5185882414113\n",
      "Scale: 1.1 Optimal Reward: -408.7265463008354, Bias: 586.995484942242\n",
      "Task C22: r: -382.12482552748196, pr: 554.5293012298127, new_pr: 172.40447570233079\n",
      "Scale: 1.1 Optimal Reward: -29.514712465844774, Bias: 586.995484942242\n",
      "Task C23: r: -29.877695769911732, pr: 170.22512086291556, new_pr: 140.34742509300384\n",
      "Scale: 1.1 Optimal Reward: -378.882149163024, Bias: 586.995484942242\n",
      "Task C24: r: 8.0001, pr: 128.77457467976706, new_pr: 136.77467467976706\n",
      "Scale: 1.1 Optimal Reward: -416.5644638749772, Bias: 586.995484942242\n",
      "\n",
      "Total sum of pseudo-rewards: 587.05\n",
      "\n",
      "Optimal Rewards\n",
      "Task C22 172.40447570233079\n",
      "Task C23 140.34742509300384\n",
      "Task C21 137.5185882414113\n",
      "Task C24 136.77467467976706\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C21\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C22\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C23\n",
      "*********** MDP ***********\n",
      "Solving goal as: Task C24\n",
      "*********** Scale at end ***********\n",
      "****** In Scale Bias ******\n",
      "Sum Of all Values: 6870\n",
      "Bias: 455.6183991697881\n",
      "Task A11: r: 7.00029999, pr: 447.70766933142727, new_pr: 454.70796932142724\n",
      "Scale: 1.1 Pre Scale: -7.191572580328057, Bias: 455.6183991697881 Final: 454.70796932142724\n",
      "Task A12: r: 13.0001, pr: 441.31828916978816, new_pr: 454.31838916978813\n",
      "Scale: 1.1 Pre Scale: -13.000099999999975, Bias: 455.6183991697881 Final: 454.31838916978813\n",
      "Task A13: r: 13.0001, pr: 441.31828916978816, new_pr: 454.31838916978813\n",
      "Scale: 1.1 Pre Scale: -13.000099999999975, Bias: 455.6183991697881 Final: 454.31838916978813\n",
      "Task A21: r: 17.00029999, pr: 436.50894899889, new_pr: 453.50924898888996\n",
      "Scale: 1.1 Pre Scale: -17.372227428089218, Bias: 455.6183991697881 Final: 453.50924898888996\n",
      "Task A22: r: 1.0, pr: 454.51773916978806, new_pr: 455.51773916978806\n",
      "Scale: 1.1 Pre Scale: -1.0006000000000768, Bias: 455.6183991697881 Final: 455.51773916978806\n",
      "Task A23: r: 7.0, pr: 447.91839916978813, new_pr: 454.91839916978813\n",
      "Scale: 1.1 Pre Scale: -7.0, Bias: 455.6183991697881 Final: 454.91839916978813\n",
      "Task B11: r: 76.000599960001, pr: 371.8463668054279, new_pr: 447.84696676542893\n",
      "Scale: 1.1 Pre Scale: -76.15639305850925, Bias: 455.6183991697881 Final: 447.84696676542893\n",
      "Task B12: r: 18.0001, pr: 435.81828916978816, new_pr: 453.81838916978813\n",
      "Scale: 1.1 Pre Scale: -18.000099999999975, Bias: 455.6183991697881 Final: 453.81838916978813\n",
      "Task B21: r: 18.0001, pr: 435.81828916978816, new_pr: 453.81838916978813\n",
      "Scale: 1.1 Pre Scale: -18.000099999999975, Bias: 455.6183991697881 Final: 453.81838916978813\n",
      "Task B22: r: 8.0001, pr: 446.8127903796342, new_pr: 454.81289037963415\n",
      "Scale: 1.1 Pre Scale: -8.005098900139956, Bias: 455.6183991697881 Final: 454.81289037963415\n",
      "Task B23: r: 9.0, pr: 445.7183989498102, new_pr: 454.7183989498102\n",
      "Scale: 1.1 Pre Scale: -9.000000199979922, Bias: 455.6183991697881 Final: 454.7183989498102\n",
      "Task B24: r: 30.004498800209973, pr: 421.85905020422155, new_pr: 451.86354900443155\n",
      "Scale: 1.1 Pre Scale: -30.690317241424168, Bias: 455.6183991697881 Final: 451.86354900443155\n",
      "Task B25: r: 18.0001, pr: 435.81828916978816, new_pr: 453.81838916978813\n",
      "Scale: 1.1 Pre Scale: -18.000099999999975, Bias: 455.6183991697881 Final: 453.81838916978813\n",
      "Task C11: r: 4.0, pr: 451.21839916978814, new_pr: 455.21839916978814\n",
      "Scale: 1.1 Pre Scale: -4.0, Bias: 455.6183991697881 Final: 455.21839916978814\n",
      "Task C12: r: 3.0000999999999998, pr: 452.25757260537597, new_pr: 455.25767260537594\n",
      "Scale: 1.1 Pre Scale: -3.055296876738339, Bias: 455.6183991697881 Final: 455.25767260537594\n",
      "Task C21: r: 0.12230423008826818, pr: 6.019198238869137, new_pr: 6.141502468957405\n",
      "Scale: 1.1 Pre Scale: -408.7265463008354, Bias: 455.6183991697881 Final: 6.141502468957405\n",
      "Task C22: r: -382.12482552748196, pr: 423.15221545735886, new_pr: 41.027389929876904\n",
      "Scale: 1.1 Pre Scale: -29.514712465844774, Bias: 455.6183991697881 Final: 41.027389929876904\n",
      "Task C23: r: -29.877695769911732, pr: 38.84803509046168, new_pr: 8.970339320549947\n",
      "Scale: 1.1 Pre Scale: -378.882149163024, Bias: 455.6183991697881 Final: 8.970339320549947\n",
      "Task C24: r: 8.0001, pr: -2.602511092686825, new_pr: 5.397588907313175\n",
      "Scale: 1.1 Pre Scale: -416.5644638749772, Bias: 455.6183991697881 Final: 5.397588907313175\n",
      "Optimal Rewards\n",
      "Task: Task A22, Reward: 1.0, PRS: 455.518\n",
      "Task: Task C12, Reward: 3.0, PRS: 455.258\n",
      "Task: Task C11, Reward: 4.0, PRS: 455.218\n",
      "Task: Task A23, Reward: 7.0, PRS: 454.918\n",
      "Task: Task B22, Reward: 8.0, PRS: 454.813\n",
      "Task: Task B23, Reward: 9.0, PRS: 454.718\n",
      "Task: Task A11, Reward: 7.0, PRS: 454.708\n",
      "Task: Task A13, Reward: 13.0, PRS: 454.318\n",
      "Task: Task A12, Reward: 13.0, PRS: 454.318\n",
      "Task: Task B12, Reward: 18.0, PRS: 453.818\n",
      "Task: Task B21, Reward: 18.0, PRS: 453.818\n",
      "Task: Task B25, Reward: 18.0, PRS: 453.818\n",
      "Task: Task A21, Reward: 17.0, PRS: 453.509\n",
      "Task: Task B24, Reward: 30.004, PRS: 451.864\n",
      "Task: Task B11, Reward: 76.001, PRS: 447.847\n",
      "Task: Task C22, Reward: -382.125, PRS: 41.027\n",
      "Task: Task C23, Reward: -29.878, PRS: 8.97\n",
      "Task: Task C21, Reward: 0.122, PRS: 6.142\n",
      "Task: Task C24, Reward: 8.0, PRS: 5.398\n",
      "Net PR Sum: 6870.000000000003\n"
     ]
    }
   ],
   "source": [
    "main.solve(flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol = {\"12\": 0}\n",
    "next(iter(pol.values()))\n",
    "next(iter(pol.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-99b87e490ae2>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-99b87e490ae2>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sum([goal.value for goal in main.tree[\"Root\"]])\n",
    "sum([task.intrinsic_reward for task in main.tasks])\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_val = [-0, -0]\n",
    "i = [\"aa\", \"aa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for _,x in sorted(zip(pr_val, i))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An impossible task will never see the negative goal value effect\n",
    "Check exp_reward function\n",
    "check prs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
